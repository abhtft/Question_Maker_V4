from langchain_community.document_loaders import PyPDFLoader  
from langchain_text_splitters import RecursiveCharacterTextSplitter  
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI  
from langchain_community.vectorstores import FAISS  
from langchain_core.prompts import PromptTemplate  
import os  
from dotenv import load_dotenv  
from typing import Dict, List, Any, Tuple, Optional  
import logging  
import tiktoken  
import json  
import re  
import hashlib
from datetime import datetime

# Load environment variables  
load_dotenv()  
  
# Configure logging  
logging.basicConfig(level=logging.INFO)  
logger = logging.getLogger(__name__)  
  
# -------------------------------  
# Utility: Robust JSON parser  
# ------------------------------- 
#  
def safe_json_loads(text: str, default: Any = None) -> Any:  
    """  
    Safely parse JSON from a string. Handles ```json fenced code and extracts  
    the first valid JSON object found. Returns `default` on failure.  
    """  
    try:  
        # Ensure text is a string
        if not isinstance(text, str):
            text = str(text)
            
        # Remove markdown fences  
        if text.strip().startswith("```"):  
            parts = text.strip().split("```")  
            if len(parts) >= 2:  
                text = parts[1]  
        # Remove leading 'json'  
        if text.strip().lower().startswith("json"):  
            text = text.strip()[4:]  
        text = text.strip()  
  
        # Try direct load  
        return json.loads(text)  
    except json.JSONDecodeError:  
        # Try regex extraction  
        match = re.search(r"\{[\s\S]*\}", text)  
        if match:  
            try:  
                return json.loads(match.group(0))  
            except json.JSONDecodeError as e:  
                logger.error(f"Regex JSON parse failed: {e}")  
        logger.error("Failed to parse JSON; returning default.")  
        return default  

# -------------------------------  
# Enhanced Document Processor with Smart Chunking  
# -------------------------------  
class DocumentProcessor:  
    def __init__(self):  
        self.embeddings = AzureOpenAIEmbeddings(  
            azure_deployment='text-embedding-3-large',  
            api_version=os.getenv('AZURE_OPENAI_API_VERSION', '2024-02-15-preview'),  
            azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),  
            api_key=os.getenv('AZURE_OPENAI_API_KEY'),  
        )  
        
        # Enhanced text splitters for different content types
        self.text_splitters = {
            'default': RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                length_function=len,
                separators=["\n\n", "\n", " ", ""]
            ),
            'mathematics': RecursiveCharacterTextSplitter(
                chunk_size=800,  # Smaller chunks for math (formulas, equations)
                chunk_overlap=150,
                length_function=len,
                separators=["\n\n", "\n", " ", ""]
            ),
            'science': RecursiveCharacterTextSplitter(
                chunk_size=1200,  # Larger chunks for science concepts
                chunk_overlap=250,
                length_function=len,
                separators=["\n\n", "\n", " ", ""]
            ),
            'literature': RecursiveCharacterTextSplitter(
                chunk_size=1500,  # Larger chunks for literature
                chunk_overlap=300,
                length_function=len,
                separators=["\n\n", "\n", " ", ""]
            )
        }

    def _detect_content_type(self, text: str) -> str:
        """Detect content type based on text characteristics"""
        text_lower = text.lower()
        
        # Mathematics indicators
        math_indicators = ['equation', 'formula', 'calculate', 'solve', 'mathematics', 'math', 'algebra', 'geometry', 'trigonometry', 'calculus', '+', '-', '*', '/', '=', 'âˆš', 'Ï€', 'âˆ«', 'âˆ‘']
        math_score = sum(1 for indicator in math_indicators if indicator in text_lower)
        
        # Science indicators
        science_indicators = ['experiment', 'hypothesis', 'theory', 'molecule', 'atom', 'cell', 'organism', 'physics', 'chemistry', 'biology', 'laboratory', 'observation', 'conclusion']
        science_score = sum(1 for indicator in science_indicators if indicator in text_lower)
        
        # Literature indicators
        literature_indicators = ['poem', 'story', 'novel', 'character', 'plot', 'theme', 'metaphor', 'simile', 'literature', 'english', 'grammar', 'vocabulary', 'comprehension']
        literature_score = sum(1 for indicator in literature_indicators if indicator in text_lower)
        
        # Determine content type
        if math_score > max(science_score, literature_score):
            return 'mathematics'
        elif science_score > literature_score:
            return 'science'
        elif literature_score > 0:
            return 'literature'
        else:
            return 'default'

    def _enhance_metadata(self, doc, content_type: str, subject: str = None, grade: str = None) -> Dict[str, Any]:
        """Add enhanced metadata to documents"""
        metadata = doc.metadata.copy()
        metadata.update({
            'content_type': content_type,
            'subject': subject or 'unknown',
            'grade': grade or 'unknown',
            'chunk_id': hashlib.md5(doc.page_content.encode()).hexdigest()[:8],
            'processed_at': datetime.now().isoformat(),
            'word_count': len(doc.page_content.split()),
            'quality_score': self._calculate_quality_score(doc.page_content)
        })
        return metadata

    def _calculate_quality_score(self, text: str) -> float:
        """Calculate quality score for content filtering"""
        if not text or len(text.strip()) < 50:
            return 0.0
        
        # Quality indicators
        has_sentences = len([s for s in text.split('.') if len(s.strip()) > 10]) > 0
        has_paragraphs = len([p for p in text.split('\n\n') if len(p.strip()) > 50]) > 0
        has_structure = any(char in text for char in [':', '-', 'â€¢', '*'])
        
        score = 0.0
        if has_sentences: score += 0.4
        if has_paragraphs: score += 0.3
        if has_structure: score += 0.3
        
        return min(score, 1.0)

    def process_uploaded_document(self, pdf_path, persist_directory=None, subject: str = None, grade: str = None) -> Tuple[Any, List[Any]]:  
        try:  
            loader = PyPDFLoader(pdf_path)  
            pages = loader.load()  
            
            # Enhanced processing with content type detection
            enhanced_texts = []
            for page in pages:
                content_type = self._detect_content_type(page.page_content)
                splitter = self.text_splitters.get(content_type, self.text_splitters['default'])
                chunks = splitter.split_documents([page])
                
                # Add enhanced metadata to each chunk
                for chunk in chunks:
                    chunk.metadata = self._enhance_metadata(chunk, content_type, subject, grade)
                    # Filter out low-quality chunks
                    if chunk.metadata.get('quality_score', 0) > 0.3:
                        enhanced_texts.append(chunk)
            
            logger.info(f"Processed PDF '{pdf_path}' into {len(enhanced_texts)} quality chunks (filtered from {sum(len(splitter.split_documents([page])) for page in pages)} total chunks)")
            
            vectorstore = FAISS.from_documents(  
                documents=enhanced_texts,  
                embedding=self.embeddings  
            )  
  
            if persist_directory:  
                vectorstore.save_local(persist_directory)  
            else:  
                vectorstore.save_local("./faiss_index")  
  
            return vectorstore, enhanced_texts  
        except Exception as e:  
            logger.error(f"Error processing document: {str(e)}")  
            raise  

# -------------------------------  
# Enhanced Context Retrieval System  
# -------------------------------  
class EnhancedContextRetriever:
    def __init__(self, vectorstore: Any):
        self.vectorstore = vectorstore
        
    def _build_semantic_query(self, topic_data: Dict[str, Any]) -> str:
        """Build enhanced semantic query based on topic data"""
        subject = topic_data.get('subjectName', '').lower()
        section = topic_data.get('sectionName', '').lower()
        difficulty = topic_data.get('difficulty', '').lower()
        bloom_level = topic_data.get('bloomLevel', '').lower()
        grade = topic_data.get('classGrade', '').lower()
        
        # Enhanced query building with subject-specific terms
        query_parts = []
        
        # Core topic
        if section:
            query_parts.append(section)
        
        # Subject-specific enhancements
        if 'mathematics' in subject or 'math' in subject:
            query_parts.extend(['mathematics', 'mathematical', 'calculation', 'problem solving'])
        elif 'science' in subject:
            query_parts.extend(['scientific', 'experiment', 'theory', 'concept'])
        elif 'english' in subject or 'literature' in subject:
            query_parts.extend(['literature', 'comprehension', 'grammar', 'vocabulary'])
        elif 'history' in subject:
            query_parts.extend(['historical', 'event', 'period', 'civilization'])
        elif 'geography' in subject:
            query_parts.extend(['geographical', 'location', 'region', 'environment'])
        
        # Difficulty-specific terms
        if difficulty == 'easy':
            query_parts.extend(['basic', 'fundamental', 'introductory'])
        elif difficulty == 'hard':
            query_parts.extend(['advanced', 'complex', 'challenging'])
        
        # Bloom's taxonomy terms
        bloom_terms = {
            'remember': ['recall', 'memorize', 'identify', 'define'],
            'understand': ['explain', 'describe', 'interpret', 'summarize'],
            'apply': ['apply', 'solve', 'use', 'implement'],
            'analyze': ['analyze', 'compare', 'contrast', 'examine'],
            'evaluate': ['evaluate', 'assess', 'judge', 'critique'],
            'create': ['create', 'design', 'develop', 'construct']
        }
        if bloom_level in bloom_terms:
            query_parts.extend(bloom_terms[bloom_level])
        
        # Grade-specific terms
        if 'grade' in grade or 'class' in grade:
            query_parts.append(grade)
        
        return ' '.join(query_parts)
    
    def _determine_search_parameters(self, topic_data: Dict[str, Any]) -> Tuple[int, int]:
        """Determine optimal search parameters based on topic complexity"""
        subject = topic_data.get('subjectName', '').lower()
        difficulty = topic_data.get('difficulty', '').lower()
        bloom_level = topic_data.get('bloomLevel', '').lower()
        
        # Base parameters
        k_docs = 4
        max_tokens = 1000
        
        # Adjust based on complexity
        if difficulty == 'hard':
            k_docs = 6
            max_tokens = 1500
        elif difficulty == 'easy':
            k_docs = 3
            max_tokens = 800
        
        # Adjust based on Bloom's level
        if bloom_level in ['analyze', 'evaluate', 'create']:
            k_docs = max(k_docs, 5)
            max_tokens = max(max_tokens, 1200)
        
        # Adjust based on subject complexity
        if 'mathematics' in subject:
            k_docs = max(k_docs, 5)  # Math needs more context for formulas
        elif 'science' in subject:
            k_docs = max(k_docs, 4)  # Science needs balanced context
        
        return k_docs, max_tokens
    
    def get_enhanced_context(self, topic_data: Dict[str, Any]) -> str:
        """Get enhanced context using improved retrieval strategies"""
        try:
            # Build semantic query
            semantic_query = self._build_semantic_query(topic_data)
            logger.info(f"Enhanced semantic query: {semantic_query}")
            
            # Determine search parameters
            k_docs, max_tokens = self._determine_search_parameters(topic_data)
            logger.info(f"Search parameters: k={k_docs}, max_tokens={max_tokens}")
            
            # Perform enhanced similarity search
            docs = self.vectorstore.similarity_search(
                semantic_query,
                k=k_docs
            )
            
            # Combine and rank documents
            combined_content = self._combine_and_rank_documents(docs, topic_data)
            
            # Truncate to token limit
            context = self._truncate_to_tokens(combined_content, max_tokens)
            
            logger.info(f"Retrieved context length: {len(context)} characters")
            return context
            
        except Exception as e:
            logger.error(f"Error in enhanced context retrieval: {e}")
            return ""
    
    def _combine_and_rank_documents(self, docs: List[Any], topic_data: Dict[str, Any]) -> str:
        """Combine documents with intelligent ranking"""
        if not docs:
            return ""
        
        # Score documents based on relevance
        scored_docs = []
        for doc in docs:
            score = self._calculate_document_relevance(doc, topic_data)
            scored_docs.append((score, doc))
        
        # Sort by relevance score
        scored_docs.sort(key=lambda x: x[0], reverse=True)
        
        # Combine content with priority to higher-scored documents
        combined_parts = []
        for score, doc in scored_docs:
            if score > 0.3:  # Only include relevant documents
                combined_parts.append(doc.page_content.strip())
        
        return "\n\n".join(combined_parts)
    
    def _calculate_document_relevance(self, doc: Any, topic_data: Dict[str, Any]) -> float:
        """Calculate relevance score for a document"""
        content = doc.page_content.lower()
        metadata = doc.metadata
        
        score = 0.0
        
        # Subject match
        if topic_data.get('subjectName', '').lower() in content:
            score += 0.3
        
        # Section match
        if topic_data.get('sectionName', '').lower() in content:
            score += 0.4
        
        # Metadata quality
        if metadata.get('quality_score', 0) > 0.5:
            score += 0.2
        
        # Content type match
        subject = topic_data.get('subjectName', '').lower()
        content_type = metadata.get('content_type', 'default')
        if ('mathematics' in subject and content_type == 'mathematics') or \
           ('science' in subject and content_type == 'science') or \
           ('english' in subject and content_type == 'literature'):
            score += 0.1
        
        return min(score, 1.0)
    
    def _truncate_to_tokens(self, text: str, max_tokens: int, model: str = "gpt-4") -> str:
        """Truncate text to token limit"""
        try:
            enc = tiktoken.encoding_for_model(model)
            tokens = enc.encode(text)
            if len(tokens) <= max_tokens:
                return text
            
            truncated_tokens = tokens[:max_tokens]
            return enc.decode(truncated_tokens)
        except Exception as e:
            logger.error(f"Error in token truncation: {e}")
            # Fallback to character-based truncation
            return text[:max_tokens * 4]  # Rough approximation

# -------------------------------  
# Question Quality Verifier  
# -------------------------------  

class QuestionQualityVerifier:  
    def __init__(self):  
        self.llm = AzureChatOpenAI(  
            azure_deployment=os.getenv('AZURE_OPENAI_CHAT_DEPLOYMENT', 'gpt-4.1'),  
            api_version=os.getenv('AZURE_OPENAI_API_VERSION', '2024-02-15-preview'),  
            temperature=0,  
            azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),  
            api_key=os.getenv('AZURE_OPENAI_API_KEY'),  
        )  
  
        # âœ… Fixed: Properly escaped curly braces for LangChain PromptTemplate
        self.verification_template = """  
You are an expert educational assessment evaluator with deep knowledge of Bloom's taxonomy, difficulty calibration, and subject-specific pedagogy.  
  
You will receive:  
- A set of generated questions  
- The intended subject, grade level, topic, difficulty, and Bloom's taxonomy level  
- The original context (learning material)  
  
**Question Details:**
Subject: {subject}
Grade: {class_grade}
Topic: {topic}
Difficulty: {difficulty}
Bloom's Level: {bloom_level}
Question Type: {question_type}

**Context:**
{context}

**Questions to Evaluate:**
{questions}

Your task:  
1. Evaluate the questions for:  
   - **Relevance**: Do they match the provided context and topic?  
   - **Difficulty Alignment**: Do they match the specified difficulty level?  
   - **Bloom's Taxonomy Alignment**: Do they match the specified cognitive level?  
   - **Subject & Grade Appropriateness**  
   - **Overall Quality**: Clarity, correctness, and completeness.  
  
2. Provide scores (0â€“100) for each category.  
3. Identify **specific issues** (if any).  
4. Provide **improvement suggestions**.  
5. Give an **overall verdict**: ACCEPTED or REJECTED.  
6. Output must be valid JSON with the exact schema:  
  
{{
  "overall_verdict": "ACCEPTED" | "REJECTED",  
  "confidence_score": <integer>,  
  "detailed_feedback": {{
    "relevance_score": <integer>,  
    "difficulty_alignment": <integer>,  
    "bloom_taxonomy_alignment": <integer>,  
    "subject_grade_alignment": <integer>,  
    "overall_quality": <integer>  
  }},  
  "specific_issues": ["..."],  
  "improvement_suggestions": ["..."]  
}}  
  
Do not include any text outside the JSON.  
"""  
  
        self.prompt = PromptTemplate(  
            input_variables=[  
                "context", "questions", "subject", "class_grade", "topic",  
                "difficulty", "bloom_level", "question_type"  
            ],  
            template=self.verification_template  
        )  

        logger.info(f"Verification prompt: {self.prompt}")
  
        self.chain = self.prompt | self.llm  
  
    def verify_questions(self, questions: Dict[str, Any], topic_data: Dict[str, Any], context: str) -> Dict[str, Any]:  
        try:  
            # Ensure topic_data is a dictionary and has required keys
            if not isinstance(topic_data, dict):
                logger.error(f"topic_data is not a dictionary: {type(topic_data)}")
                topic_data = {}
                
            questions_text = json.dumps(questions, indent=2)  
            response = self.chain.invoke({  
                "context": context,  
                "questions": questions_text,  
                "subject": topic_data.get('subjectName', 'Unknown'),  
                "class_grade": topic_data.get('classGrade', 'Unknown'),  
                "topic": topic_data.get('sectionName', 'Unknown'),  
                "difficulty": topic_data.get('difficulty', 'Unknown'),  
                "bloom_level": topic_data.get('bloomLevel', 'Unknown'),  
                "question_type": topic_data.get('questionType', 'Unknown')  
            })  
  
            llm_output = response.content if hasattr(response, 'content') else str(response)  
            logger.debug(f"Raw verifier output:\n{llm_output}")  
  
            verification_result = safe_json_loads(llm_output, default={})  
  
            # âœ… Always return a dict  
            if not isinstance(verification_result, dict):  
                logger.warning("Verifier returned non-dict; using fallback ACCEPTED result.")  
                verification_result = {  
                    "overall_verdict": "ACCEPTED",  
                    "confidence_score": 80,  
                    "detailed_feedback": {  
                        "relevance_score": 80,  
                        "difficulty_alignment": 80,  
                        "bloom_taxonomy_alignment": 80,  
                        "subject_grade_alignment": 80,  
                        "overall_quality": 80  
                    },  
                    "specific_issues": [],  
                    "improvement_suggestions": []  
                }  
  
            logger.info(f"Verification result: {verification_result.get('overall_verdict', 'UNKNOWN')}")  
            return verification_result  
  
        except Exception as e:  
            logger.error(f"Error in question verification: {e}")  
            return {  
                "overall_verdict": "ACCEPTED",  
                "confidence_score": 70,  
                "detailed_feedback": {  
                    "relevance_score": 70,  
                    "difficulty_alignment": 70,  
                    "bloom_taxonomy_alignment": 70,  
                    "subject_grade_alignment": 70,  
                    "overall_quality": 70  
                },  
                "specific_issues": ["Verification process encountered an error"],  
                "improvement_suggestions": ["Consider manual review of generated questions"]  
            }  

# -------------------------------  
# Question Generator  
# -------------------------------  
class QuestionGenerator:  
    def __init__(self):  
        self.llm = AzureChatOpenAI(  
            azure_deployment=os.getenv('AZURE_OPENAI_CHAT_DEPLOYMENT', 'gpt-4.1'),  
            api_version=os.getenv('AZURE_OPENAI_API_VERSION', '2024-02-15-preview'),  
            temperature=0.0,  # Lower temp for more predictable JSON  
            azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),  
            api_key=os.getenv('AZURE_OPENAI_API_KEY'),  
        )  
  
        # ======== Your Original Question Prompt ========  
        self.question_template = """  
You are a highly skilled educational question generator.   
Generate exactly {num_questions} {question_type} questions for:  
Subject: {subject}  
Grade: {class_grade}  
Topic: {topic}  
Difficulty: {difficulty}  
Bloom's Level: {bloom_level}  
  
Context:  
{context}  
  
Additional Instructions:  
{instructions}  
  
ðŸŽ¯ Output Format (Strict JSON):
{{
"questions": [
    {{
    "question": "Your question text here.",
    "options": ["Option A", "Option B", "Option C", "Option D"],
    "answer": "Correct option here",
    "explanation": "Detailed explanation with reasoning."
    }}
]
}} 
  
output must not be in backticks and must be in json format.Please not write just json.
correct json format is given above.




Rules:  
- Each question must have exactly 4 options.  
- The answer must match one of the options exactly.  
- The explanation must justify why the answer is correct.  
- No extra text outside JSON.  
"""  
  
        # ======== Your Original Revision Prompt ========  
        self.revision_template = """  
You are a highly skilled educational question generator.  
You previously generated questions that did not meet quality requirements.  
  
Context:  
{context}  
  
Original Issues:  
{quality_issues}  
  
Suggestions:  
{improvement_suggestions}  
  
Specific Improvements:  
{specific_improvements}  
  
Generate exactly {num_questions} {question_type} questions for:  
Subject: {subject}  
Grade: {class_grade}  
Topic: {topic}  
Difficulty: {difficulty}  
Bloom's Level: {bloom_level}  
  
Instructions:  
{instructions}  
  
ðŸŽ¯ Output Format (Strict JSON):
{{
"questions": [
    {{
    "question": "Your question text here.",
    "options": ["Option A", "Option B", "Option C", "Option D"],
    "answer": "Correct option here",
    "explanation": "Detailed explanation with reasoning."
    }}
]
}} 
  
output must not be in backticks and must be in json format.Please not write just json.
correct json format is given above.

"""  
  
        self.prompt = PromptTemplate(  
            input_variables=[  
                "context", "num_questions", "question_type", "subject",  
                "class_grade", "topic", "difficulty", "bloom_level", "instructions"  
            ],  
            template=self.question_template  
        )  
  
        self.revision_prompt = PromptTemplate(  
            input_variables=[  
                "context", "num_questions", "question_type", "subject",  
                "class_grade", "topic", "difficulty", "bloom_level", "instructions",  
                "quality_issues", "improvement_suggestions", "specific_improvements"  
            ],  
            template=self.revision_template  
        )  
  
        self.chain = self.prompt | self.llm  
        self.revision_chain = self.revision_prompt | self.llm  
  
    def generate_questions(self, topic_data: Dict[str, Any], vectorstore: Any, verifier: QuestionQualityVerifier) -> Dict[str, Any]:  
        max_attempts = 3  
        
        # Ensure topic_data is a dictionary
        if not isinstance(topic_data, dict):
            logger.error(f"topic_data is not a dictionary: {type(topic_data)}")
            raise ValueError("topic_data must be a dictionary")
            
        context = self._get_context(topic_data, vectorstore)  
        verification_result = None  # Prevents unbound variable error  
  
        for attempt in range(max_attempts):  
            try:  
                logger.info(f"Question generation attempt {attempt + 1}/{max_attempts}")  
  
                if attempt == 0:  
                    response = self.chain.invoke({  
                        "context": context,  
                        "num_questions": topic_data.get('numQuestions', 1),  
                        "question_type": topic_data.get('questionType', 'MCQ'),  
                        "subject": topic_data.get('subjectName', 'Unknown'),  
                        "class_grade": topic_data.get('classGrade', 'Unknown'),  
                        "topic": topic_data.get('sectionName', 'Unknown'),  
                        "difficulty": topic_data.get('difficulty', 'Medium'),  
                        "bloom_level": topic_data.get('bloomLevel', 'Remember'),  
                        "instructions": topic_data.get('additionalInstructions', '')  
                    })  
                else:  

                    # For revision attempts (attempt > 0), use feedback from the previous attempt
                    # verification_result contains feedback from the most recent attempt

                    quality_issues = self._format_issues(verification_result.get('specific_issues', [])) if verification_result else "Previous output was not valid JSON or missing required fields."  
                    improvement_suggestions = self._format_suggestions(verification_result.get('improvement_suggestions', [])) if verification_result else "Ensure output strictly follows the JSON schema."  
                    specific_improvements = self._format_improvements(verification_result) if verification_result else "Return only JSON with the required fields."  
  
                    response = self.revision_chain.invoke({  
                        "context": context,  
                        "num_questions": topic_data.get('numQuestions', 1),  
                        "question_type": topic_data.get('questionType', 'MCQ'),  
                        "subject": topic_data.get('subjectName', 'Unknown'),  
                        "class_grade": topic_data.get('classGrade', 'Unknown'),  
                        "topic": topic_data.get('sectionName', 'Unknown'),  
                        "difficulty": topic_data.get('difficulty', 'Medium'),  
                        "bloom_level": topic_data.get('bloomLevel', 'Remember'),  
                        "instructions": topic_data.get('additionalInstructions', ''),  
                        "quality_issues": quality_issues,  
                        "improvement_suggestions": improvement_suggestions,  
                        "specific_improvements": specific_improvements  
                    })  
  
                result = self._parse_llm_response(response)  
                verification_result = verifier.verify_questions(result, topic_data, context)
                logger.info(f"\nVerification result: {verification_result}\n")  
  
                if verification_result['overall_verdict'] == 'ACCEPTED':  
                    logger.info(f"Questions accepted on attempt {attempt + 1}")  
                    return {  
                        'questions': result,  
                        'verification_result': verification_result,  
                        'attempts_used': attempt + 1  
                    }  
                else:  
                    logger.info(f"Questions rejected on attempt {attempt + 1}, preparing for revision")  
                    if attempt == max_attempts - 1:  
                        logger.warning("Maximum attempts reached, returning questions despite quality issues")  
                        return {  
                            'questions': result,  
                            'verification_result': verification_result,  
                            'attempts_used': attempt + 1,  
                            'warning': 'Maximum revision attempts reached'  
                        }  
  
            except Exception as e:  
                logger.error(f"Error in attempt {attempt + 1}: {e}")  
                if attempt == max_attempts - 1:  
                    raise  
  
        raise Exception("Failed to generate questions after maximum attempts")  
  
    def _get_context(self, topic_data: Dict[str, Any], vectorstore: Any) -> str:  
        """Get enhanced context using the new EnhancedContextRetriever"""
        if not vectorstore:
            return ""
            
        try:
            # Use the enhanced context retriever
            context_retriever = EnhancedContextRetriever(vectorstore)
            context = context_retriever.get_enhanced_context(topic_data)
            
            if context:
                logger.info(f"Enhanced context retrieved: {len(context)} characters")
                logger.debug(f"Context preview: {context[:200]}...")
            else:
                logger.warning("No context retrieved from enhanced retriever")
                
            return context
            
        except Exception as e:
            logger.error(f"Error in enhanced context retrieval: {e}")
            # Fallback to basic context retrieval
            return self._get_basic_context(topic_data, vectorstore)
    
    def _get_basic_context(self, topic_data: Dict[str, Any], vectorstore: Any) -> str:
        """Fallback basic context retrieval method"""
        def truncate_to_tokens(text: str, max_tokens: int = 4000, model: str = "gpt-4") -> str:  
            enc = tiktoken.encoding_for_model(model)  
            tokens = enc.encode(text)  
            truncated_tokens = tokens[:max_tokens]  
            return enc.decode(truncated_tokens)  

        context = ""  
        if vectorstore:  
            try:  
                # Safe access to topic_data keys
                subject = topic_data.get('subjectName', '')
                section = topic_data.get('sectionName', '')
                search_query = f"{subject} {section}".strip()
                
                if not search_query:
                    search_query = "general content"
                    
                docs = vectorstore.similarity_search(  
                    search_query,  
                    k=4  
                )  
                raw_context = "\n".join(doc.page_content.strip() for doc in docs)  
                context = truncate_to_tokens(raw_context, max_tokens=1000, model="gpt-4")  
                logger.info(f"Using fallback context from vectorstore (truncated): {context[:200]}...")  
            except Exception as e:  
                logger.error(f"Error getting fallback context: {e}")  
        return context  
  
    def _parse_llm_response(self, response: Any) -> Dict[str, Any]:  
        llm_output = response.content if hasattr(response, 'content') else str(response)  
        logger.info(f"Raw LLM output: {llm_output}")  
  
        # Ensure llm_output is a string
        if not isinstance(llm_output, str):
            llm_output = str(llm_output)
        
        result = safe_json_loads(llm_output, default=None)  
        if not isinstance(result, dict) or 'questions' not in result:  
            raise ValueError("Invalid response format: missing 'questions' key or not a dict")  
  
        if not isinstance(result['questions'], list):  
            raise ValueError("'questions' must be a list")  
  
        for i, q in enumerate(result['questions']):  
            if not isinstance(q, dict):  
                raise ValueError(f"Question {i} is not a dictionary")  
            required_fields = ['question', 'options', 'answer', 'explanation']  
            missing_fields = [field for field in required_fields if field not in q]  
            if missing_fields:  
                raise ValueError(f"Question {i} missing fields: {missing_fields}")  
            if not isinstance(q['options'], list) or len(q['options']) != 4:  
                raise ValueError(f"Question {i} must have exactly 4 options")  
            if q['answer'] not in q['options']:  
                raise ValueError(f"Question {i} answer must be one of the options")  
  
        return result  
  
    def _format_issues(self, issues: List[str]) -> str:  
        return "\n".join([f"- {issue}" for issue in issues]) if issues else "No specific issues identified"  
  
    def _format_suggestions(self, suggestions: List[str]) -> str:  
        return "\n".join([f"- {s}" for s in suggestions]) if suggestions else "No specific suggestions provided"  
  
    def _format_improvements(self, verification_result: Dict[str, Any]) -> str:  
        # Ensure verification_result is a dictionary
        if not isinstance(verification_result, dict):
            return "No specific improvements required"
            
        feedback = verification_result.get('detailed_feedback', {})  
        
        # Ensure feedback is a dictionary
        if not isinstance(feedback, dict):
            return "No specific improvements required"
            
        improvements = []  
        if feedback.get('relevance_score', 100) < 70:  
            improvements.append("Improve relevance to the provided context")  
        if feedback.get('difficulty_alignment', 100) < 70:  
            improvements.append("Better align with the specified difficulty level")  
        if feedback.get('bloom_taxonomy_alignment', 100) < 70:  
            improvements.append("Better align with the specified Bloom's taxonomy level")  
        if feedback.get('subject_grade_alignment', 100) < 70:  
            improvements.append("Make questions more appropriate for the subject and grade level")  
        if feedback.get('overall_quality', 100) < 70:  
            improvements.append("Improve overall question quality and clarity")  
        return "\n".join([f"- {improvement}" for improvement in improvements]) if improvements else "No specific improvements required"  
  
  
# -------------------------------  
# Initialize components  
# -------------------------------  
document_processor = DocumentProcessor()  
question_generator = QuestionGenerator()  
question_verifier = QuestionQualityVerifier()  

# Enhanced components for Phase 1 improvements
# These will be automatically used by the existing components
# No changes needed to app.py integration

"""
PHASE 1 IMPROVEMENTS IMPLEMENTED:
âœ… Enhanced Document Processing with Smart Chunking
âœ… Metadata-Enhanced Storage with Quality Filtering
âœ… Enhanced Context Retrieval with Semantic Queries
âœ… Dynamic Search Parameters based on Topic Complexity
âœ… Content Type Detection and Specialized Chunking
âœ… Document Relevance Scoring and Ranking

NEXT PHASES TO IMPLEMENT:
Phase 2: Multi-Stage Retrieval, Query Enhancement, Caching
Phase 3: Reranking with Cross-Encoders, Advanced Analytics

OUTPUT FORMAT REMAINS UNCHANGED - FULL COMPATIBILITY WITH app.py
""" 
